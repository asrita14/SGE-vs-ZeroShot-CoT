<p align="center">
  <img src="https://img.shields.io/badge/NLP-Project-9146FF?style=for-the-badge">
  <img src="https://img.shields.io/badge/Transformers-FLAN--T5-blue?style=for-the-badge">
  <img src="https://img.shields.io/badge/Status-Complete-brightgreen?style=for-the-badge">
</p>

<h1 align="center">ğŸ¤– Self-Generated Examples vs Zero-shot and CoT Prompting</h1>

<p align="center">
A comparative study of prompting strategies across several reasoning benchmarks using FLAN-T5.
</p>

---

# ğŸŒŸ Overview

This project evaluates how different prompting strategies behave across reasoning tasks:

### **Prompting Methods**
- **Zero-shot**  
- **Few-shot**, manually written  
- **Chain-of-Thought (CoT)**  
- **CoT Self-Consistency (CoT-SC)**  
- **Self-Generated Examples (SGE)**  
- **Filtered SGE (our new method)** âœ”ï¸

### **Datasets**
- **GSM8K** â€” math word problems  
- **BoolQ** â€” yes/no QA  
- **CommonsenseQA** â€” 5-way multiple choice  

We measure performance, stability, and behavior across:
- **task types**
- **difficulty levels (easy vs hard)**  
- **SGE quality (filtered vs unfiltered)**

---

#  Key Contributions

âœ”ï¸ Introduce **Filtered SGE**, a scoring-based filtering mechanism for synthetic examples  
âœ”ï¸ Add **CoT-Self-Consistency** for more stable math reasoning  
âœ”ï¸ Design a unified evaluation pipeline  
âœ”ï¸ Provide difficulty-aware analysis across datasets  
âœ”ï¸ Produce reproducible metrics and plots  

---

# Installation

```bash
git clone https://github.com/asrita14/SGE-vs-ZeroShot-CoT.git
cd SGE-vs-ZeroShot-CoT
conda create -n prompting-env python=3.10 -y
conda activate prompting-env
pip install -r requirements.txt
```
# Repository Structure
```graphql
SGE-vs-ZeroShot-CoT/
â”‚
â”œâ”€â”€ README.md                 # High-level overview (this file)
â”œâ”€â”€ make_plots.py             # Generates all project figures
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ README.md             # How to run experiments, metrics, analysis
â”‚   â”œâ”€â”€ eval_prompting.py     # Main evaluation driver
â”‚   â”œâ”€â”€ compute_metrics.py    # Metrics computation
â”‚   â”œâ”€â”€ difficulty_analysis.py# Easy/Hard tagging + analysis
â”‚   â””â”€â”€ prompting/
â”‚       â”œâ”€â”€ README.md         # Explanation of prompting methods
â”‚       â”œâ”€â”€ baselines.py      # Zero-shot, Few-shot, CoT, CoT-SC
â”‚       â””â”€â”€ sge_pipeline.py   # SGE + SGE filtering implementation
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ README.md             # What the JSONs / figs mean
    â”œâ”€â”€ metrics/              # Raw & summarized JSON outputs
    â””â”€â”€ figs/                 # Plots used in report + poster
```
# High-Level Results

| Dataset   | Best Method  | Observation                                          |
| --------- | ------------ | ---------------------------------------------------- |
| **BoolQ** | SGE-Filtered | Slightly improves over Zero-shot; CoT harms accuracy |
| **CSQA**  | Zero-shot    | Strong commonsense priors already in FLAN-T5         |
| **GSM8K** | CoT-SC       | Best stability; large MAE reduction                  |

# Reproducibility Checklist

âœ” Uses only HF datasets
âœ” Single model class (FLAN-T5)
âœ” All outputs saved as JSON
âœ” Metrics reproducible by one command
âœ” Figures reproducible by one command

3 Acknowledgements

NYU DS-GA 1011 â€” Natural Language Processing
HuggingFace Transformers
FLAN-T5 Instruction-Tuned Model

# Contact

For issues or reproducibility questions, open a GitHub issue.
